name: Daily Web Crawler

on:
  schedule:
    - cron: '0 0 * * *'  # 每天 UTC 凌晨0點執行一次

jobs:
  run-crawler:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repo
      uses: actions/checkout@v3

    - name: Cache pip
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: pip install -r requirements.txt

    - name: Run crawler
      run: python crawler.py

    - name: Commit and push results
      run: |
        git config --global user.name "GitHub Action"
        git config --global user.email "action@github.com"
        git add output.txt
        git commit -m "Update output.txt at $(date '+%Y-%m-%d %H:%M')" || echo "No changes"
        git push